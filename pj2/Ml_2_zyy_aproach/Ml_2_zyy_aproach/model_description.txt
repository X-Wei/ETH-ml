recently, Google open-sourced their machine learning tools, Tensorflow. In this project, we applied our model which is called softmax using Tensorflow.
The idea of naive softmax model is straightforward. It estimates the probability of three classes separately with respect to each single data point, then assigns the class which scores highest probability to the data point.
The loss function we use is cross-entropy, it is defined as (http://tensorflow.org/tutorials/mnist/beginners/index.md 搜索cross-entropy, 那个公式). y_i_prime denotes the true distribution in the training data, y_i is our estimated result.
Then gradient descent is conducted to train the model. 

The original data was rooted, and standarlized using Z-score method, and performed PCA by a dimension reduction to 4 or 5. Then we generate a laplace kernel to resample the input data onto a high dimensional space, as the input data of the soft-max model.
We use 575 data points to train our model, and other 100 points are used for model validation.
The best result of our model is about 0.94(fig.1). This result is generated by a model without PCA and with a kernel of 3000 dimension.  